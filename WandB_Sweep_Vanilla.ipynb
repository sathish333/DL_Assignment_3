{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:41:33.606840Z",
     "iopub.status.busy": "2023-05-20T15:41:33.606502Z",
     "iopub.status.idle": "2023-05-20T15:43:07.238823Z",
     "shell.execute_reply": "2023-05-20T15:43:07.237840Z",
     "shell.execute_reply.started": "2023-05-20T15:41:33.606811Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "train,valid,test=load_data(data_path,lang)\n",
    "\n",
    "add_start_end(train) #adding start and end characters\n",
    "add_start_end(valid)\n",
    "add_start_end(test)\n",
    "\n",
    "train_src_chars,train_target_chars=get_unique_chars(train) # obtain unique charcaters\n",
    "valid_src_chars,valid_target_chars=get_unique_chars(valid)\n",
    "test_src_chars,test_target_chars=get_unique_chars(test)\n",
    "train_target_chars.add('*') # extra char to handle unknowns in valid and test data.\n",
    "    \n",
    "src_char_idx,src_idx_char=get_char_map(train_src_chars) # create map for each unique charcter to -> integer\n",
    "target_char_idx,target_idx_char=get_char_map(train_target_chars)\n",
    "\n",
    "encoder_vocab_size=len(src_char_idx)+1 # one extra for padding\n",
    "decoder_vocab_size=len(target_char_idx)+1 # one extra for padding\n",
    "\n",
    "max_seq_length=train[0].apply(lambda x:len(x)).max() # maximum sequence lenght in Latin\n",
    "max_target_length=train[1].apply(lambda x:len(x)).max() # maximum target length\n",
    "\n",
    "\n",
    "#creating word vectors\n",
    "train_src_int,train_target_int=vectorize(train,src_char_idx,target_char_idx,max_seq_length)\n",
    "valid_src_int,valid_target_int=vectorize(valid,src_char_idx,target_char_idx,max_seq_length)\n",
    "test_src_int,test_target_int=vectorize(test,src_char_idx,target_char_idx,max_seq_length)\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    this methid will be called with different configuration from WandB sweep.\n",
    "    '''\n",
    "    wandb.init()\n",
    "    config=wandb.config \n",
    "    run_name=f'Cell-{config.cell_type} Hidden-{config.hidden_size} Embedding-{config.embedding_size} Bidir-{config.bidirectional} Dropout -{config.dropout} EL-{config.encoder_num_layers} DL-{config.decoder_num_layers}'\n",
    "    wandb.run.name=run_name\n",
    "    config.encoder_vocab_size=encoder_vocab_size\n",
    "    config.decoder_vocab_size=decoder_vocab_size\n",
    "    config.max_seq_length=max_seq_length\n",
    "\n",
    "    model=Seq2Seq(config).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss=0\n",
    "        train_acc=0\n",
    "        model.train()\n",
    "        batch_no=0\n",
    "        for data in get_batch(train_src_int,train_target_int,config.batch_size):\n",
    "#             print(batch_no)\n",
    "            batch_no+=1\n",
    "            x=data[0]\n",
    "            y=data[1]\n",
    "            x=x.to(torch.int64).T\n",
    "            y=y.to(torch.int64).T\n",
    "            outputs,_=model.forward(x,y)\n",
    "            output=outputs.reshape(-1,outputs.shape[2])\n",
    "            target=y.reshape(-1)\n",
    "            optimizer.zero_grad()\n",
    "            target=target-1\n",
    "            target[target<0]=0\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # gradient clipping \n",
    "            optimizer.step() # update parameters\n",
    "            train_loss+=loss.item()*config.batch_size\n",
    "\n",
    "            batch_acc=cal_acc(outputs,y)\n",
    "            train_acc+=batch_acc\n",
    "        train_loss/=len(train_src_int)\n",
    "        train_acc/=batch_no \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss=0\n",
    "        valid_acc=0\n",
    "        batch_no=0\n",
    "        with torch.no_grad():# disable storing computation graph\n",
    "            for data in get_batch(valid_src_int,valid_target_int,config.batch_size):\n",
    "                batch_no+=1\n",
    "                x=data[0]\n",
    "                y=data[1]\n",
    "                x=x.to(torch.int64).T\n",
    "                y=y.to(torch.int64).T\n",
    "                outputs,_=model.forward(x,y,prediction=True) # prediction set to True to disable teacher forcing\n",
    "                output=outputs.reshape(-1,outputs.shape[2])\n",
    "                target=y.reshape(-1)\n",
    "                target=target-1\n",
    "                target[target<0]=0\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss+=loss.item()*config.batch_size\n",
    "                valid_acc+=cal_acc(outputs,y)\n",
    "            valid_loss/=len(valid_src_int)\n",
    "            valid_acc/=batch_no\n",
    "        print(f'Epoch: {epoch+1} Train Loss: {train_loss:.4f} Valid Loss: {valid_loss:.4f} Train Acc: {train_acc:.4f}  Valid Acc: {valid_acc:.4f}')\n",
    "        wandb.log({'train accuracy':train_acc,'train loss':train_loss,'valid accuracy':valid_acc,'valid loss':valid_loss})\n",
    "    wandb.finish()\n",
    "    return model\n",
    "\n",
    "\n",
    "sweep_config= {\n",
    "    'method': 'bayes',\n",
    "    'name': 'Vanilla Sweep',\n",
    "    'metric': {\n",
    "        'goal': 'maximize', \n",
    "        'name': 'valid accuracy'\n",
    "      },\n",
    "    \"parameters\":\n",
    "    {\n",
    "        'hidden_size': {\"values\":[32,64,128,256]},\n",
    "        'batch_size': {\"values\":[64,128,256]},\n",
    "        'encoder_num_layers': {\"values\":[1,2,3]},\n",
    "        'decoder_num_layers': {\"values\":[1,2,3]},\n",
    "        'embedding_size': {\"values\":[128,256]},\n",
    "        'dropout': {\"values\":[0,0.2,0.3]},\n",
    "        'epochs':{\"values\":[15]},\n",
    "        'cell_type':{\"values\":['LSTM',\"GRU\",\"RNN\"]},\n",
    "        'bidirectional':{\"values\":[\"Yes\",\"No\"]}\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='DL_3_Assign')\n",
    "wandb.agent(sweep_id, main, count=100)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:44:17.611671Z",
     "iopub.status.busy": "2023-05-20T15:44:17.611193Z",
     "iopub.status.idle": "2023-05-20T15:44:17.616756Z",
     "shell.execute_reply": "2023-05-20T15:44:17.615829Z",
     "shell.execute_reply.started": "2023-05-20T15:44:17.611633Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
